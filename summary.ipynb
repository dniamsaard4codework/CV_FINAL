{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computer Vision Final Exam Summary\n",
        "\n",
        "## Comprehensive Lab Works Summary\n",
        "\n",
        "This notebook consolidates all lab works from:\n",
        "- **CNN** (Convolutional Neural Networks)\n",
        "- **OD** (Object Detection - YOLO)\n",
        "- **OT** (Object Tracking)\n",
        "- **IS** (Image Segmentation)\n",
        "- **GenAI** (Generative Models - GAN & VAE)\n",
        "- **3D Vision** (PyTorch3D, Open3D, PointNet)\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [CNN - Convolutional Neural Networks](#cnn)\n",
        "2. [Object Detection - YOLO](#yolo)\n",
        "3. [Object Tracking](#tracking)\n",
        "4. [Image Segmentation](#segmentation)\n",
        "5. [Generative Models](#generative)\n",
        "6. [3D Vision](#3dvision)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. CNN - Convolutional Neural Networks {#cnn}\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### 1.1 Linear Classifiers\n",
        "\n",
        "**Components:**\n",
        "- **Weights and Bias Initialization**: Random small values for weights, zero for bias\n",
        "- **Matrix Multiplication**: $y = X\\theta + b$ or with augmented matrix $y = X_{bias} \\theta_{bias}$\n",
        "- **Activation Functions**:\n",
        "  - **Sigmoid** (Binary): $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
        "    - Output range: (0, 1)\n",
        "    - Used for binary classification\n",
        "  - **Softmax** (Multi-class): $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
        "    - Output: Probability distribution over K classes\n",
        "    - Sum of outputs = 1\n",
        "- **Loss Functions**:\n",
        "  - **Binary Cross-Entropy**: $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$\n",
        "  - **Categorical Cross-Entropy**: $J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})$\n",
        "- **Gradient Descent**: $\\theta = \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}$\n",
        "  - **Batch GD**: Uses all samples\n",
        "  - **Stochastic GD**: Uses one sample at a time\n",
        "  - **Mini-batch GD**: Uses small batches\n",
        "\n",
        "### 1.2 Multi-Layer Perceptron (MLP)\n",
        "\n",
        "**Architecture:**\n",
        "- Input Layer → Hidden Layer(s) with ReLU → Output Layer with Softmax\n",
        "- **ReLU**: $f(x) = \\max(0, x)$\n",
        "  - Advantages: Non-saturating, sparse activations\n",
        "  - Problem: Dead neurons (gradient = 0 for x < 0)\n",
        "- **Backpropagation**: Chain rule for gradient computation through layers\n",
        "- **Vanishing Gradient**: Problem in deep networks (solved by ReLU, BatchNorm)\n",
        "\n",
        "### 1.3 Convolutional Neural Networks (CNN)\n",
        "\n",
        "**Key Components:**\n",
        "- **Convolutional Layers**: Extract spatial features using filters\n",
        "  - **Filter/Kernel**: Small matrix that slides over input\n",
        "  - **Feature Maps**: Output of convolution operation\n",
        "  - **Stride**: Step size of filter movement\n",
        "  - **Padding**: Add zeros around input to preserve size\n",
        "- **Pooling Layers**: Reduce spatial dimensions\n",
        "  - **MaxPool**: Takes maximum value in window\n",
        "  - **AvgPool**: Takes average value in window\n",
        "  - **Benefits**: Reduces parameters, provides translation invariance\n",
        "- **Fully Connected Layers**: Final classification\n",
        "- **Activation Functions**: ReLU, Softmax\n",
        "\n",
        "**Output Size Formula:**\n",
        "$$\\text{Output Size} = \\frac{W - K + 2P}{S} + 1$$\n",
        "where:\n",
        "- $W$ = input width/height\n",
        "- $K$ = kernel size\n",
        "- $P$ = padding\n",
        "- $S$ = stride\n",
        "\n",
        "**Example Calculation:**\n",
        "- Input: 28×28, Kernel: 3×3, Padding: 0, Stride: 1\n",
        "- Output: $\\frac{28 - 3 + 2(0)}{1} + 1 = 26$\n",
        "- After MaxPool(2): $\\frac{26}{2} = 13$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Implementation Example\n",
        "\n",
        "### Simple CNN Architecture\n",
        "```\n",
        "Input (1, 28, 28) \n",
        "→ Conv2d(1, 16, kernel_size=3) + ReLU \n",
        "→ MaxPool2d(2) \n",
        "→ Flatten \n",
        "→ Linear(13*13*16, 10) \n",
        "→ Output (10 classes)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple CNN Model\n",
        "class MyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCNN, self).__init__()\n",
        "        # Conv layer: (28-3+2*0)/1 + 1 = 26\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
        "        # MaxPool: 26/2 = 13\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        # FC layer: 13*13*16 = 2704\n",
        "        self.fc1 = nn.Linear(13*13*16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        return x, F.log_softmax(x, dim=1)\n",
        "\n",
        "# Model initialization\n",
        "cnn_model = MyCNN().to(device)\n",
        "print(cnn_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer Learning with ResNet\n",
        "\n",
        "**Key Points:**\n",
        "- Use pre-trained models (e.g., ResNet-18) on ImageNet\n",
        "- Modify final layer for your number of classes\n",
        "- Options:\n",
        "  - **Fine-tuning**: Update all weights\n",
        "  - **Feature Extraction**: Freeze pre-trained layers, train only final layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet-18 Transfer Learning Example\n",
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "# Load pre-trained ResNet-18\n",
        "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify final layer for 10 classes (e.g., CIFAR-10)\n",
        "num_classes = 10\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "\n",
        "# For feature extraction (freeze pre-trained layers):\n",
        "# for param in resnet18.parameters():\n",
        "#     param.requires_grad = False\n",
        "# resnet18.fc.requires_grad = True  # Only train final layer\n",
        "\n",
        "resnet18 = resnet18.to(device)\n",
        "print(f\"ResNet-18 modified for {num_classes} classes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Object Detection - YOLO {#yolo}\n",
        "\n",
        "## YOLO (You Only Look Once) Architecture\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**YOLO v1 Architecture:**\n",
        "- **Input**: 448×448 image\n",
        "- **Output**: 7×7 grid, each cell predicts:\n",
        "  - 2 bounding boxes (B=2)\n",
        "  - Confidence scores\n",
        "  - 20 class probabilities (C=20 for VOC)\n",
        "  - **Total output**: $S \\times S \\times (C + B \\times 5) = 7 \\times 7 \\times 30 = 1470$\n",
        "\n",
        "**Bounding Box Format:**\n",
        "- $(x, y, w, h)$: center coordinates and dimensions (normalized 0-1)\n",
        "- Confidence: $P(\\text{object}) \\times \\text{IoU}$\n",
        "\n",
        "**Key Features:**\n",
        "- **Single Forward Pass**: Detects objects in one pass (faster than R-CNN)\n",
        "- **Grid-based**: Divides image into S×S grid\n",
        "- **Multi-scale**: Can detect objects of different sizes\n",
        "\n",
        "### Loss Function Components\n",
        "\n",
        "**Total Loss = Coordinate Loss + Confidence Loss + Class Loss**\n",
        "\n",
        "1. **Box Coordinate Loss**: \n",
        "   $$\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2]$$\n",
        "   \n",
        "2. **Box Size Loss** (with square root):\n",
        "   $$\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]$$\n",
        "   *Note: Square root makes small boxes and large boxes contribute equally*\n",
        "   \n",
        "3. **Object Confidence Loss**:\n",
        "   $$\\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2$$\n",
        "   \n",
        "4. **No Object Confidence Loss**:\n",
        "   $$\\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2$$\n",
        "   *Note: $\\lambda_{noobj} = 0.5$ to reduce penalty for background*\n",
        "   \n",
        "5. **Class Loss**:\n",
        "   $$\\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c \\in classes} (p_i(c) - \\hat{p}_i(c))^2$$\n",
        "\n",
        "**Loss Weights:**\n",
        "- $\\lambda_{coord} = 5$ (emphasize coordinate accuracy)\n",
        "- $\\lambda_{noobj} = 0.5$ (reduce background penalty)\n",
        "\n",
        "### Post-Processing\n",
        "\n",
        "**Non-Maximum Suppression (NMS):**\n",
        "1. Sort boxes by confidence score\n",
        "2. Select box with highest confidence\n",
        "3. Remove all boxes with IoU > threshold\n",
        "4. Repeat until no boxes remain\n",
        "\n",
        "**Key Metrics:**\n",
        "- **IoU (Intersection over Union)**: $\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}$\n",
        "- **mAP (mean Average Precision)**: Average precision across all classes\n",
        "- **Precision**: $\\frac{TP}{TP + FP}$\n",
        "- **Recall**: $\\frac{TP}{TP + FN}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO Architecture Config\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3),      # (kernel_size, filters, stride, padding)\n",
        "    \"M\",                 # MaxPool\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # Repeated 4 times\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # Repeated 2 times\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "]\n",
        "\n",
        "# CNN Block\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "# Complete YOLO Model\n",
        "class Yolov1(nn.Module):\n",
        "    def __init__(self, in_channels=3, split_size=7, num_boxes=2, num_classes=20):\n",
        "        super(Yolov1, self).__init__()\n",
        "        self.architecture = architecture_config\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = self._create_conv_layers(self.architecture)\n",
        "        self.fcs = self._create_fc(split_size, num_boxes, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))\n",
        "    \n",
        "    def _create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "        \n",
        "        for x in architecture:\n",
        "            if type(x) == tuple:\n",
        "                layers += [CNNBlock(in_channels, out_channels=x[1], \n",
        "                                  kernel_size=x[0], stride=x[2], padding=x[3])]\n",
        "                in_channels = x[1]\n",
        "            elif type(x) == str:\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            elif type(x) == list:\n",
        "                conv1 = x[0]\n",
        "                conv2 = x[1]\n",
        "                num_repeats = x[2]\n",
        "                for _ in range(num_repeats):\n",
        "                    layers += [CNNBlock(in_channels, out_channels=conv1[1],\n",
        "                                      kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
        "                    layers += [CNNBlock(conv1[1], out_channels=conv2[1],\n",
        "                                      kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
        "                    in_channels = conv2[1]\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def _create_fc(self, split_size, num_boxes, num_classes):\n",
        "        S, B, C = split_size, num_boxes, num_classes\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024*S*S, 4096),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, S*S*(C + B*5)),\n",
        "        )\n",
        "\n",
        "# IoU Calculation\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    Calculate IoU between predicted and ground truth boxes\n",
        "    \"\"\"\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "    elif box_format == \"corners\":\n",
        "        box1_x1, box1_y1 = boxes_preds[..., 0:1], boxes_preds[..., 1:2]\n",
        "        box1_x2, box1_y2 = boxes_preds[..., 2:3], boxes_preds[..., 3:4]\n",
        "        box2_x1, box2_y1 = boxes_labels[..., 0:1], boxes_labels[..., 1:2]\n",
        "        box2_x2, box2_y2 = boxes_labels[..., 2:3], boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "    \n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "# Non-Maximum Suppression\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Apply NMS to remove overlapping bounding boxes\n",
        "    \"\"\"\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "    \n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "        bboxes = [\n",
        "            box for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            ) < iou_threshold\n",
        "        ]\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "    \n",
        "    return bboxes_after_nms\n",
        "\n",
        "print(\"YOLO components defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Object Tracking Examples\n",
        "\n",
        "# 1. Color-Based Tracking\n",
        "from collections import deque\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def color_based_tracking(frame, color_lower, color_upper, buffer_size=64):\n",
        "    \"\"\"\n",
        "    Track object by color in HSV space\n",
        "    \"\"\"\n",
        "    # Convert to HSV\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    \n",
        "    # Create mask\n",
        "    mask = cv2.inRange(hsv, color_lower, color_upper)\n",
        "    mask = cv2.erode(mask, None, iterations=2)\n",
        "    mask = cv2.dilate(mask, None, iterations=2)\n",
        "    \n",
        "    # Find contours\n",
        "    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "    \n",
        "    center = None\n",
        "    if len(cnts) > 0:\n",
        "        # Find largest contour\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "        ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
        "        M = cv2.moments(c)\n",
        "        if M[\"m00\"] != 0:\n",
        "            center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
        "    \n",
        "    return center, mask\n",
        "\n",
        "# 2. Single Object Tracking\n",
        "def single_object_tracking_example():\n",
        "    \"\"\"\n",
        "    Single object tracking using OpenCV trackers\n",
        "    \"\"\"\n",
        "    tracker_types = ['BOOSTING', 'MIL', 'KCF', 'GOTURN']\n",
        "    tracker_type = 'KCF'\n",
        "    \n",
        "    # Create tracker\n",
        "    if tracker_type == 'BOOSTING':\n",
        "        tracker = cv2.legacy.TrackerBoosting_create()\n",
        "    elif tracker_type == 'MIL':\n",
        "        tracker = cv2.legacy.TrackerMIL_create()\n",
        "    elif tracker_type == 'KCF':\n",
        "        tracker = cv2.legacy.TrackerKCF_create()\n",
        "    elif tracker_type == 'GOTURN':\n",
        "        tracker = cv2.legacy.TrackerGOTURN_create()\n",
        "    \n",
        "    video = cv2.VideoCapture(0)\n",
        "    ok, frame = video.read()\n",
        "    if not ok:\n",
        "        return\n",
        "    \n",
        "    bbox = cv2.selectROI(frame, False)\n",
        "    ok = tracker.init(frame, bbox)\n",
        "    \n",
        "    while True:\n",
        "        ok, frame = video.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        \n",
        "        ok, bbox = tracker.update(frame)\n",
        "        \n",
        "        if ok:\n",
        "            p1 = (int(bbox[0]), int(bbox[1]))\n",
        "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
        "            cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)\n",
        "        else:\n",
        "            cv2.putText(frame, \"Tracking failure\", (100, 80),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
        "        \n",
        "        cv2.imshow(\"Tracking\", frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    \n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# 3. Multi-Object Tracking\n",
        "def multi_object_tracking_example():\n",
        "    \"\"\"\n",
        "    Multi-object tracking using MultiTracker\n",
        "    \"\"\"\n",
        "    tracker_type = \"KCF\"\n",
        "    \n",
        "    if hasattr(cv2, \"legacy\"):\n",
        "        FACTORIES = {\n",
        "            \"KCF\": cv2.legacy.TrackerKCF_create,\n",
        "            \"MIL\": cv2.legacy.TrackerMIL_create,\n",
        "        }\n",
        "        trackers = cv2.legacy.MultiTracker_create()\n",
        "    else:\n",
        "        FACTORIES = {\n",
        "            \"KCF\": getattr(cv2, \"TrackerKCF_create\", None),\n",
        "            \"MIL\": getattr(cv2, \"TrackerMIL_create\", None),\n",
        "        }\n",
        "        trackers = cv2.MultiTracker_create()\n",
        "    \n",
        "    video = cv2.VideoCapture('video.mp4')\n",
        "    \n",
        "    while True:\n",
        "        ok, frame = video.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        \n",
        "        frame = cv2.resize(frame, (720, 640))\n",
        "        success, boxes = trackers.update(frame)\n",
        "        \n",
        "        if success:\n",
        "            for b in boxes:\n",
        "                x, y, w, h = map(int, b)\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        else:\n",
        "            cv2.putText(frame, \"Tracking failure\", (100, 80),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
        "        \n",
        "        cv2.imshow(\"Frame\", frame)\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        \n",
        "        if key == ord(\"s\"):\n",
        "            # Add new tracker\n",
        "            box = cv2.selectROI(\"Frame\", frame, fromCenter=False, showCrosshair=True)\n",
        "            tr = FACTORIES[tracker_type]()\n",
        "            trackers.add(tr, frame, box)\n",
        "        elif key == ord(\"q\"):\n",
        "            break\n",
        "    \n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# 4. Non-Maximum Suppression (NMS) for Tracking\n",
        "def nms_boxes(boxes, iou_thr=0.3):\n",
        "    \"\"\"\n",
        "    Apply Non-Maximum Suppression to remove overlapping boxes\n",
        "    \"\"\"\n",
        "    if not boxes:\n",
        "        return []\n",
        "    \n",
        "    b = np.array(boxes, dtype=np.float32)\n",
        "    x1, y1 = b[:, 0], b[:, 1]\n",
        "    x2, y2 = b[:, 0] + b[:, 2], b[:, 1] + b[:, 3]\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = areas.argsort()[::-1]\n",
        "    \n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        iw = np.maximum(0.0, xx2 - xx1)\n",
        "        ih = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = iw * ih\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)\n",
        "        order = order[1:][iou < iou_thr]\n",
        "    \n",
        "    return [tuple(map(int, b[i])) for i in keep]\n",
        "\n",
        "print(\"Tracking functions defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Image Segmentation {#segmentation}\n",
        "\n",
        "## Segmentation Methods\n",
        "\n",
        "### 4.1 Classical Methods\n",
        "\n",
        "#### Mean Shift Clustering\n",
        "- **Non-parametric clustering algorithm**\n",
        "- Groups pixels by iteratively shifting towards higher density\n",
        "- Good for color-based segmentation\n",
        "- **Bandwidth**: Controls cluster size (smaller = more clusters)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Estimate bandwidth from data\n",
        "2. For each point, shift towards mean of points in bandwidth window\n",
        "3. Iterate until convergence\n",
        "4. Merge nearby modes\n",
        "\n",
        "#### K-Means Clustering\n",
        "- **Partition pixels into K clusters**\n",
        "- Minimize within-cluster variance\n",
        "- Fast and simple\n",
        "- **K**: Number of clusters (must be specified)\n",
        "\n",
        "**Algorithm:**\n",
        "1. Initialize K cluster centers (random or K-means++)\n",
        "2. Assign each pixel to nearest center\n",
        "3. Update centers as mean of assigned pixels\n",
        "4. Repeat until convergence or max iterations\n",
        "\n",
        "**Criteria:**\n",
        "- **TERM_CRITERIA_EPS**: Stop when change < epsilon\n",
        "- **TERM_CRITERIA_MAX_ITER**: Stop after max iterations\n",
        "- **KMEANS_PP_CENTERS**: K-means++ initialization\n",
        "\n",
        "### 4.2 Deep Learning Methods\n",
        "\n",
        "#### U-Net Architecture\n",
        "\n",
        "**Key Features:**\n",
        "- **Encoder (Contracting Path)**: Downsampling, feature extraction\n",
        "  - Uses VGG16 or similar backbone\n",
        "  - Progressively reduces spatial size, increases channels\n",
        "- **Bottleneck**: Deepest layer with highest-level features\n",
        "  - Smallest spatial size, most channels\n",
        "- **Decoder (Expansive Path)**: Upsampling, spatial recovery\n",
        "  - Progressively increases spatial size, decreases channels\n",
        "  - Uses transposed convolution (deconvolution)\n",
        "- **Skip Connections**: Preserve fine-grained details\n",
        "  - Concatenate encoder features with decoder at same level\n",
        "  - Helps recover spatial information lost in downsampling\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Input (256×256) \n",
        "→ Encoder Block 1 (128×128) ────┐\n",
        "→ Encoder Block 2 (64×64) ──────┤\n",
        "→ Encoder Block 3 (32×32) ──────┤\n",
        "→ Encoder Block 4 (16×16) ──────┤\n",
        "→ Encoder Block 5 (8×8) ────────┤\n",
        "→ Bottleneck (8×8)               │\n",
        "→ Decoder Block 1 (16×16) ←─────┘ (skip from Block 4)\n",
        "→ Decoder Block 2 (32×32) ←─────── (skip from Block 3)\n",
        "→ Decoder Block 3 (64×64) ←─────── (skip from Block 2)\n",
        "→ Decoder Block 4 (128×128) ←───── (skip from Block 1)\n",
        "→ Output (256×256)\n",
        "```\n",
        "\n",
        "**Loss Function:**\n",
        "- **Cross-Entropy Loss**: For pixel-wise classification\n",
        "  - $L_{CE} = -\\sum_{i} y_i \\log(\\hat{y}_i)$\n",
        "- **Dice Loss**: For imbalanced classes\n",
        "  - $L_{Dice} = 1 - \\frac{2|X \\cap Y|}{|X| + |Y|}$\n",
        "- **Combined Loss**: $L = L_{CE} + \\lambda L_{Dice}$\n",
        "\n",
        "**Key Components:**\n",
        "- **Conv Block**: Conv2d → BatchNorm → ReLU\n",
        "- **Up Conv**: Transposed Convolution (Deconvolution)\n",
        "  - Increases spatial dimensions\n",
        "  - Can have checkerboard artifacts\n",
        "- **Skip Connections**: Concatenate encoder features with decoder\n",
        "  - Channel dimension doubles: `torch.cat([decoder_feat, encoder_feat], dim=1)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classical Segmentation Methods\n",
        "\n",
        "# 1. K-Means Segmentation\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def kmeans_segmentation(image, K=4):\n",
        "    \"\"\"\n",
        "    K-Means segmentation example\n",
        "    \"\"\"\n",
        "    # Reshape image to 2D array of pixels\n",
        "    img2D = image.reshape((-1, 3))\n",
        "    img2D = np.float32(img2D)\n",
        "    \n",
        "    # Define criteria\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    \n",
        "    # Apply K-Means\n",
        "    ret, label, center = cv2.kmeans(img2D, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
        "    \n",
        "    # Convert back to uint8\n",
        "    center = np.uint8(center)\n",
        "    res = center[label.flatten()]\n",
        "    result = res.reshape(image.shape)\n",
        "    \n",
        "    return result, label, center\n",
        "\n",
        "# 2. Mean Shift Segmentation\n",
        "def mean_shift_segmentation(image, quantile=0.04):\n",
        "    \"\"\"\n",
        "    Mean Shift clustering for segmentation\n",
        "    \"\"\"\n",
        "    # Resize and reshape\n",
        "    img = cv2.resize(image, None, fx=0.5, fy=0.5)\n",
        "    img2D = img.reshape((-1, 3))\n",
        "    img2D = np.float32(img2D)\n",
        "    \n",
        "    # Normalize\n",
        "    norm_img2d = MinMaxScaler(feature_range=(0, 1)).fit_transform(img2D)\n",
        "    \n",
        "    # Estimate bandwidth\n",
        "    bandwidth = estimate_bandwidth(norm_img2d, quantile=quantile, n_jobs=2)\n",
        "    \n",
        "    # Apply Mean Shift\n",
        "    ms_res = MeanShift(bandwidth=bandwidth, n_jobs=2, bin_seeding=True, cluster_all=True).fit(norm_img2d)\n",
        "    \n",
        "    # Get labels and reshape\n",
        "    labels = ms_res.labels_\n",
        "    segmented_image = labels.reshape(img.shape[:2])\n",
        "    \n",
        "    return segmented_image, ms_res.cluster_centers_\n",
        "\n",
        "# U-Net Complete Implementation\n",
        "def conv_block(in_channels, out_channels):\n",
        "    \"\"\"Convolutional block with BatchNorm and ReLU\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def up_conv_block(in_channels, out_channels):\n",
        "    \"\"\"Up-convolution block\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "# Complete U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, pretrained=True, out_channels=12):\n",
        "        super().__init__()\n",
        "        # Use VGG16 as encoder\n",
        "        self.encoder = models.vgg16_bn(pretrained=pretrained).features\n",
        "        self.block1 = nn.Sequential(*self.encoder[:6])\n",
        "        self.block2 = nn.Sequential(*self.encoder[6:13])\n",
        "        self.block3 = nn.Sequential(*self.encoder[13:20])\n",
        "        self.block4 = nn.Sequential(*self.encoder[20:27])\n",
        "        self.block5 = nn.Sequential(*self.encoder[27:34])\n",
        "        \n",
        "        self.bottleneck = nn.Sequential(*self.encoder[34:])\n",
        "        self.conv_bottleneck = conv_block(512, 1024)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        self.up_conv6 = up_conv_block(1024, 512)\n",
        "        self.conv6 = conv_block(512 + 512, 512)\n",
        "        self.up_conv7 = up_conv_block(512, 256)\n",
        "        self.conv7 = conv_block(256 + 512, 256)\n",
        "        self.up_conv8 = up_conv_block(256, 128)\n",
        "        self.conv8 = conv_block(128 + 256, 128)\n",
        "        self.up_conv9 = up_conv_block(128, 64)\n",
        "        self.conv9 = conv_block(64 + 128, 64)\n",
        "        self.up_conv10 = up_conv_block(64, 32)\n",
        "        self.conv10 = conv_block(32 + 64, 32)\n",
        "        self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        \n",
        "        bottleneck = self.bottleneck(block5)\n",
        "        x = self.conv_bottleneck(bottleneck)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.up_conv6(x)\n",
        "        x = torch.cat([x, block5], dim=1)\n",
        "        x = self.conv6(x)\n",
        "        \n",
        "        x = self.up_conv7(x)\n",
        "        x = torch.cat([x, block4], dim=1)\n",
        "        x = self.conv7(x)\n",
        "        \n",
        "        x = self.up_conv8(x)\n",
        "        x = torch.cat([x, block3], dim=1)\n",
        "        x = self.conv8(x)\n",
        "        \n",
        "        x = self.up_conv9(x)\n",
        "        x = torch.cat([x, block2], dim=1)\n",
        "        x = self.conv9(x)\n",
        "        \n",
        "        x = self.up_conv10(x)\n",
        "        x = torch.cat([x, block1], dim=1)\n",
        "        x = self.conv10(x)\n",
        "        \n",
        "        x = self.conv11(x)\n",
        "        return x\n",
        "\n",
        "# U-Net Loss Function\n",
        "def unet_loss(preds, targets):\n",
        "    \"\"\"\n",
        "    U-Net loss: Cross-entropy for pixel-wise classification\n",
        "    \"\"\"\n",
        "    targets = targets.squeeze(1).long()\n",
        "    ce_loss = nn.CrossEntropyLoss()(preds, targets)\n",
        "    acc = (torch.max(preds, 1)[1] == targets).float().mean()\n",
        "    return ce_loss, acc\n",
        "\n",
        "print(\"Segmentation functions defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Generative Models {#generative}\n",
        "\n",
        "## 5.1 Generative Adversarial Networks (GAN)\n",
        "\n",
        "### Architecture\n",
        "\n",
        "**Two Networks:**\n",
        "1. **Generator (G)**: Creates fake images from noise\n",
        "   - Input: Random noise vector $z \\sim \\mathcal{N}(0, 1)$ or $\\mathcal{U}(-1, 1)$\n",
        "   - Output: Generated image $G(z)$\n",
        "   - Goal: Fool discriminator into thinking generated images are real\n",
        "   \n",
        "2. **Discriminator (D)**: Distinguishes real from fake\n",
        "   - Input: Real or generated image\n",
        "   - Output: Probability of being real $D(x) \\in [0, 1]$\n",
        "   - Goal: Correctly classify real vs fake\n",
        "\n",
        "### Training Process\n",
        "\n",
        "**Minimax Game:**\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
        "\n",
        "**Training Steps:**\n",
        "1. **Train Discriminator** (k steps, usually k=1):\n",
        "   - Real images: maximize $\\log D(x)$ → minimize $-\\log D(x)$\n",
        "   - Fake images: maximize $\\log(1 - D(G(z)))$ → minimize $-\\log(1 - D(G(z)))$\n",
        "   - Combined: $L_D = -\\frac{1}{2}[\\log D(x) + \\log(1 - D(G(z)))]$\n",
        "   \n",
        "2. **Train Generator** (1 step):\n",
        "   - **Saturating**: Minimize $\\log(1 - D(G(z)))$ (can have vanishing gradients)\n",
        "   - **Non-saturating**: Maximize $\\log D(G(z))$ → minimize $-\\log D(G(z))$ (better)\n",
        "   - Use flipped labels: treat fake as real\n",
        "\n",
        "**Loss Functions:**\n",
        "- **Discriminator Loss**: $L_D = -\\frac{1}{2}[\\log D(x) + \\log(1 - D(G(z)))]$\n",
        "- **Generator Loss**: $L_G = -\\log D(G(z))$ (non-saturating)\n",
        "\n",
        "**Training Tips:**\n",
        "- Train D and G alternately\n",
        "- Use same learning rate for both (typically 0.0002)\n",
        "- Use Adam optimizer with betas=(0.5, 0.9)\n",
        "- Label smoothing: Use 0.9 instead of 1.0 for real labels\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Generator:**\n",
        "- Fully connected or convolutional layers\n",
        "- Batch normalization (except input layer)\n",
        "- ReLU/LeakyReLU activations\n",
        "- Tanh output (normalized to [-1, 1])\n",
        "- No dropout in generator\n",
        "\n",
        "**Discriminator:**\n",
        "- Binary classifier\n",
        "- Sigmoid output\n",
        "- Dropout for regularization (p=0.3-0.5)\n",
        "- LeakyReLU (negative slope=0.2)\n",
        "\n",
        "## 5.2 Variational Autoencoders (VAE)\n",
        "\n",
        "### Architecture\n",
        "\n",
        "**Encoder-Decoder Structure:**\n",
        "- **Encoder**: Maps input $x$ to latent distribution parameters $(\\mu, \\sigma^2)$\n",
        "  - Outputs: $\\mu$ (mean) and $\\log(\\sigma^2)$ (log variance)\n",
        "  - Why log? Ensures $\\sigma^2 > 0$ (always positive)\n",
        "- **Reparameterization Trick**: Sample $z = \\mu + \\sigma \\cdot \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$\n",
        "  - **Why needed?**: Allows backpropagation through random sampling\n",
        "  - Without it: sampling is non-differentiable\n",
        "  - With it: $\\frac{\\partial z}{\\partial \\mu} = 1$, $\\frac{\\partial z}{\\partial \\sigma} = \\epsilon$\n",
        "- **Decoder**: Reconstructs $x$ from latent $z$\n",
        "  - Output: Reconstructed image $\\hat{x}$\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "**VAE Loss = Reconstruction Loss + KL Divergence**\n",
        "\n",
        "$$L_{VAE} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))$$\n",
        "\n",
        "**Components:**\n",
        "1. **Reconstruction Loss**: \n",
        "   - MSE: $||x - \\hat{x}||^2$ (for continuous data)\n",
        "   - BCE: $-\\sum [x \\log(\\hat{x}) + (1-x)\\log(1-\\hat{x})]$ (for binary data)\n",
        "   - Measures how well decoder reconstructs input\n",
        "   \n",
        "2. **KL Divergence**: Regularizes latent space to standard normal $\\mathcal{N}(0, I)$\n",
        "   - Ensures latent space is well-structured\n",
        "   - Allows smooth interpolation\n",
        "   - Formula: $D_{KL} = -\\frac{1}{2} \\sum_{i=1}^{d} [1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2]$\n",
        "\n",
        "**Interpretation:**\n",
        "- **Reconstruction Term**: Maximize likelihood of data\n",
        "- **KL Term**: Minimize distance from prior (standard normal)\n",
        "- **Trade-off**: More KL weight → better structure, worse reconstruction\n",
        "\n",
        "### Key Differences: GAN vs VAE\n",
        "\n",
        "| Aspect | GAN | VAE |\n",
        "|--------|-----|-----|\n",
        "| Training | Adversarial (minimax game) | Reconstruction + Regularization |\n",
        "| Latent Space | No explicit structure | Structured (Gaussian) |\n",
        "| Generation Quality | High quality, sharp | Blurrier, smoother |\n",
        "| Interpolation | Not guaranteed smooth | Smooth in latent space |\n",
        "| Mode Collapse | Can occur | Less common |\n",
        "| Training Stability | Can be unstable | More stable |\n",
        "| Use Case | High-quality generation | Latent space exploration |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GAN Generator Example\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, img_dim=(1, 28, 28)):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        def block(in_feat, out_feat, normalize=True, dropout=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(p=0.5))\n",
        "            return layers\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            *block(self.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            nn.Linear(512, int(np.prod(img_dim))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *self.img_dim)\n",
        "        return img\n",
        "\n",
        "# GAN Discriminator Example\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_dim=(1, 28, 28)):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_dim = img_dim\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(self.img_dim)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n",
        "# VAE Example\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, stride=2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(32, 64, stride=2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        \n",
        "        self.z_mean = nn.Linear(3136, 2)  # 7x7x64 = 3136\n",
        "        self.z_log_var = nn.Linear(3136, 2)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, 3136),\n",
        "            nn.Unflatten(1, (64, 7, 7)),\n",
        "            nn.ConvTranspose2d(64, 64, stride=2, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.ConvTranspose2d(64, 32, stride=2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.ConvTranspose2d(32, 1, stride=2, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def reparameterize(self, z_mu, z_log_var):\n",
        "        \"\"\"Reparameterization trick\"\"\"\n",
        "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.device)\n",
        "        z = z_mu + eps * torch.exp(z_log_var / 2.)\n",
        "        return z\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, z_mean, z_log_var, decoded\n",
        "\n",
        "print(\"Generative models defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch3D Mesh Rendering\n",
        "try:\n",
        "    import pytorch3d\n",
        "    import pytorch3d.renderer\n",
        "    import pytorch3d.structures\n",
        "    import pytorch3d.io\n",
        "    \n",
        "    def setup_mesh_renderer(device='cuda'):\n",
        "        \"\"\"Setup mesh renderer with camera and lights\"\"\"\n",
        "        # Camera setup\n",
        "        R = torch.eye(3).unsqueeze(0)  # Identity rotation\n",
        "        T = torch.tensor([[0, 0, 3]])  # Camera at z=3\n",
        "        \n",
        "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
        "            R=R, T=T, fov=60, device=device\n",
        "        )\n",
        "        \n",
        "        # Renderer setup\n",
        "        image_size = 512\n",
        "        raster_settings = pytorch3d.renderer.RasterizationSettings(\n",
        "            image_size=image_size\n",
        "        )\n",
        "        rasterizer = pytorch3d.renderer.MeshRasterizer(\n",
        "            raster_settings=raster_settings\n",
        "        )\n",
        "        shader = pytorch3d.renderer.HardPhongShader(device=device)\n",
        "        renderer = pytorch3d.renderer.MeshRenderer(\n",
        "            rasterizer=rasterizer, shader=shader\n",
        "        )\n",
        "        \n",
        "        return renderer, cameras\n",
        "    \n",
        "    def load_and_render_mesh(obj_path, device='cuda'):\n",
        "        \"\"\"Load mesh and render it\"\"\"\n",
        "        # Load mesh\n",
        "        vertices, face_props, text_props = pytorch3d.io.load_obj(obj_path)\n",
        "        faces = face_props.verts_idx\n",
        "        \n",
        "        # Batch the tensors\n",
        "        vertices = vertices.unsqueeze(0).to(device)\n",
        "        faces = faces.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Create texture\n",
        "        texture_rgb = torch.ones_like(vertices) * torch.tensor([0.7, 0.7, 1]).to(device)\n",
        "        textures = pytorch3d.renderer.TexturesVertex(texture_rgb)\n",
        "        \n",
        "        # Create mesh\n",
        "        meshes = pytorch3d.structures.Meshes(\n",
        "            verts=vertices, faces=faces, textures=textures\n",
        "        )\n",
        "        \n",
        "        # Setup renderer\n",
        "        renderer, cameras = setup_mesh_renderer(device)\n",
        "        \n",
        "        # Render\n",
        "        image = renderer(meshes, cameras=cameras)\n",
        "        return image[0].cpu().numpy()\n",
        "    \n",
        "    print(\"PyTorch3D functions defined\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch3D not available\")\n",
        "\n",
        "# Open3D Examples\n",
        "try:\n",
        "    import open3d as o3d\n",
        "    \n",
        "    def open3d_mesh_operations():\n",
        "        \"\"\"Open3D mesh processing examples\"\"\"\n",
        "        # Load mesh\n",
        "        mesh = o3d.io.read_triangle_mesh(\"mesh.obj\")\n",
        "        \n",
        "        # Compute normals\n",
        "        mesh.compute_vertex_normals()\n",
        "        \n",
        "        # Mesh filtering\n",
        "        mesh_smooth = mesh.filter_smooth_simple(number_of_iterations=5)\n",
        "        mesh_laplacian = mesh.filter_smooth_laplacian(number_of_iterations=10)\n",
        "        \n",
        "        # Point sampling\n",
        "        pcd = mesh.sample_points_uniformly(number_of_points=500)\n",
        "        \n",
        "        # Mesh properties\n",
        "        is_watertight = mesh.is_watertight()\n",
        "        is_orientable = mesh.is_orientable()\n",
        "        \n",
        "        return mesh, pcd\n",
        "    \n",
        "    print(\"Open3D functions defined\")\n",
        "except ImportError:\n",
        "    print(\"Open3D not available\")\n",
        "\n",
        "# Complete PointNet Implementation\n",
        "class Tnet(nn.Module):\n",
        "    \"\"\"Transform network for PointNet\"\"\"\n",
        "    def __init__(self, k=3):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k*k)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        bs = input.size(0)\n",
        "        xb = F.relu(self.bn1(self.conv1(input)))\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "        pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        flat = nn.Flatten(1)(pool)\n",
        "        xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "        xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "        \n",
        "        # Initialize as identity\n",
        "        init = torch.eye(self.k, requires_grad=True).repeat(bs, 1, 1)\n",
        "        if xb.is_cuda:\n",
        "            init = init.cuda()\n",
        "        matrix = self.fc3(xb).view(-1, self.k, self.k) + init\n",
        "        return matrix\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    \"\"\"Complete PointNet for classification\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        \n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        \n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # Input transform\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        xb = torch.bmm(torch.transpose(input, 1, 2), matrix3x3).transpose(1, 2)\n",
        "        \n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "        \n",
        "        # Feature transform\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb, 1, 2), matrix64x64).transpose(1, 2)\n",
        "        \n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        \n",
        "        # Classification head\n",
        "        xb = F.relu(self.bn4(self.fc1(output)))\n",
        "        xb = F.relu(self.bn5(self.dropout(self.fc2(xb))))\n",
        "        output = self.fc3(xb)\n",
        "        \n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64\n",
        "\n",
        "# PointNet Loss\n",
        "def pointnet_loss(outputs, labels, m3x3, m64x64, alpha=0.0001):\n",
        "    \"\"\"PointNet loss with transform regularization\"\"\"\n",
        "    criterion = nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)\n",
        "\n",
        "print(\"3D Vision components defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Key Formulas\n",
        "\n",
        "### CNN\n",
        "- **Output Size**: $\\frac{W - K + 2P}{S} + 1$\n",
        "- **Parameters**: $\\text{params} = (K \\times K \\times C_{in} + 1) \\times C_{out}$\n",
        "- **Pooling Output**: $\\frac{W}{S}$ (for stride S)\n",
        "\n",
        "### YOLO\n",
        "- **Output Size**: $S \\times S \\times (C + B \\times 5)$\n",
        "- **IoU**: $\\text{IoU} = \\frac{\\text{Intersection}}{\\text{Union}} = \\frac{\\text{Area}(B_1 \\cap B_2)}{\\text{Area}(B_1 \\cup B_2)}$\n",
        "- **mAP**: Mean Average Precision across all classes\n",
        "- **Confidence**: $P(\\text{object}) \\times \\text{IoU}_{pred}^{truth}$\n",
        "\n",
        "### GAN\n",
        "- **Discriminator Loss**: $L_D = -\\frac{1}{2}[\\log D(x) + \\log(1 - D(G(z)))]$\n",
        "- **Generator Loss**: $L_G = -\\log D(G(z))$ (non-saturating)\n",
        "- **Minimax Objective**: $\\min_G \\max_D V(D, G)$\n",
        "\n",
        "### VAE\n",
        "- **Total Loss**: $L = \\text{Reconstruction} + D_{KL}(q_\\phi(z|x) || p(z))$\n",
        "- **KL Divergence**: $D_{KL} = -\\frac{1}{2} \\sum_{i=1}^{d} [1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2]$\n",
        "- **Reparameterization**: $z = \\mu + \\sigma \\cdot \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$\n",
        "\n",
        "### PointNet\n",
        "- **Loss**: $L = L_{cls} + \\alpha(||I - A_3 A_3^T||_F^2 + ||I - A_{64} A_{64}^T||_F^2)$\n",
        "- **Max Pooling**: Makes network permutation invariant\n",
        "\n",
        "### Segmentation\n",
        "- **Dice Coefficient**: $\\text{Dice} = \\frac{2|X \\cap Y|}{|X| + |Y|}$\n",
        "- **Pixel Accuracy**: $\\frac{\\text{Correct Pixels}}{\\text{Total Pixels}}$\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "### Classification\n",
        "- **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "- **Precision**: $\\frac{TP}{TP + FP}$\n",
        "- **Recall**: $\\frac{TP}{TP + FN}$\n",
        "- **F1-Score**: $\\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
        "\n",
        "### Object Detection\n",
        "- **IoU**: Intersection over Union\n",
        "- **mAP**: Mean Average Precision\n",
        "- **AP@0.5**: Average Precision at IoU threshold 0.5\n",
        "\n",
        "### Segmentation\n",
        "- **Pixel Accuracy**: Percentage of correctly classified pixels\n",
        "- **Mean IoU**: Average IoU across all classes\n",
        "- **Dice Coefficient**: Overlap measure\n",
        "\n",
        "---\n",
        "\n",
        "## Training Tips & Best Practices\n",
        "\n",
        "### CNN Training\n",
        "1. **Data Augmentation**: Rotation, flipping, scaling\n",
        "2. **Batch Normalization**: Stabilizes training\n",
        "3. **Learning Rate Scheduling**: Reduce on plateau\n",
        "4. **Early Stopping**: Prevent overfitting\n",
        "\n",
        "### YOLO Training\n",
        "1. **Anchor Boxes**: Predefined box shapes\n",
        "2. **Multi-scale Training**: Train on different image sizes\n",
        "3. **Data Augmentation**: Mosaic, mixup\n",
        "4. **Loss Balancing**: $\\lambda_{coord} = 5$, $\\lambda_{noobj} = 0.5$\n",
        "\n",
        "### GAN Training\n",
        "1. **Balance**: Train D and G alternately\n",
        "2. **Learning Rates**: Usually same for both networks\n",
        "3. **Batch Normalization**: Use in generator\n",
        "4. **Label Smoothing**: Prevent discriminator from being too confident\n",
        "\n",
        "### VAE Training\n",
        "1. **KL Weight**: Balance reconstruction vs regularization\n",
        "2. **Beta-VAE**: Weight KL term for better disentanglement\n",
        "3. **Latent Dimension**: Choose based on data complexity\n",
        "\n",
        "---\n",
        "\n",
        "## Exam Tips\n",
        "\n",
        "### Key Concepts to Master\n",
        "\n",
        "1. **CNN**:\n",
        "   - Understand convolution operation and how dimensions change\n",
        "   - Know pooling effects (MaxPool, AvgPool)\n",
        "   - Transfer learning strategies\n",
        "\n",
        "2. **YOLO**:\n",
        "   - Output format: $S \\times S \\times (C + B \\times 5)$\n",
        "   - Loss function components\n",
        "   - IoU calculation and NMS\n",
        "\n",
        "3. **Tracking**:\n",
        "   - Different tracker types (KCF, MIL, etc.)\n",
        "   - Detection + tracking pipeline\n",
        "   - Multi-object tracking challenges\n",
        "\n",
        "4. **Segmentation**:\n",
        "   - U-Net architecture with skip connections\n",
        "   - Classical vs deep learning methods\n",
        "   - Loss functions for segmentation\n",
        "\n",
        "5. **GAN/VAE**:\n",
        "   - Training process (adversarial vs reconstruction)\n",
        "   - Loss functions and their meanings\n",
        "   - Reparameterization trick for VAE\n",
        "\n",
        "6. **3D Vision**:\n",
        "   - Mesh rendering pipeline\n",
        "   - PointNet architecture and permutation invariance\n",
        "   - Transform networks\n",
        "\n",
        "### Practice Problems\n",
        "\n",
        "1. **Calculate CNN output size**: Given input 224×224, kernel 3×3, padding 1, stride 1\n",
        "2. **Calculate YOLO output**: Given S=7, C=20, B=2\n",
        "3. **Compute IoU**: Given two bounding boxes\n",
        "4. **Implement forward pass**: For any architecture\n",
        "5. **Explain skip connections**: Why they're important in U-Net\n",
        "\n",
        "### Common Mistakes to Avoid\n",
        "\n",
        "1. **Dimension Mismatches**: Always check tensor shapes\n",
        "2. **Loss Function**: Use correct loss for task (CE for classification, MSE for regression)\n",
        "3. **Batch Dimension**: Remember to batch all tensors\n",
        "4. **Device Mismatch**: Ensure all tensors on same device (CPU/GPU)\n",
        "5. **Gradient Flow**: Check if requires_grad is set correctly\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "### PyTorch Essentials\n",
        "```python\n",
        "# Model definition\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Linear(10, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for batch in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input)\n",
        "```\n",
        "\n",
        "### Common Operations\n",
        "- **Flatten**: `torch.flatten(x, start_dim=1)`\n",
        "- **Concatenate**: `torch.cat([x1, x2], dim=1)`\n",
        "- **Reshape**: `x.view(batch, channels, height, width)`\n",
        "- **Transpose**: `x.transpose(1, 2)` or `x.permute(0, 2, 1)`\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck with your exam!** 🎓\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Training Loop Examples\n",
        "\n",
        "### CNN Training Loop\n",
        "```python\n",
        "def train_cnn(model, train_loader, val_loader, epochs, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs, _ = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_losses.append(train_loss/len(train_loader))\n",
        "        train_accs.append(100*train_correct/train_total)\n",
        "        val_losses.append(val_loss/len(val_loader))\n",
        "        val_accs.append(100*val_correct/val_total)\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "        print(f'Train Loss: {train_losses[-1]:.4f}, Acc: {train_accs[-1]:.2f}%')\n",
        "        print(f'Val Loss: {val_losses[-1]:.4f}, Acc: {val_accs[-1]:.2f}%')\n",
        "    \n",
        "    return train_losses, train_accs, val_losses, val_accs\n",
        "```\n",
        "\n",
        "### GAN Training Loop\n",
        "```python\n",
        "def train_gan(generator, discriminator, train_loader, epochs, device):\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (real_images, _) in enumerate(train_loader):\n",
        "            batch_size = real_images.size(0)\n",
        "            real_images = real_images.to(device)\n",
        "            real_labels = torch.ones(batch_size, device=device)\n",
        "            fake_labels = torch.zeros(batch_size, device=device)\n",
        "            \n",
        "            # Train Discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            \n",
        "            # Real images\n",
        "            discr_pred_real = discriminator(real_images).view(-1)\n",
        "            real_loss = adversarial_loss(discr_pred_real, real_labels)\n",
        "            \n",
        "            # Fake images\n",
        "            z = torch.randn(batch_size, 100, device=device)\n",
        "            fake_images = generator(z)\n",
        "            discr_pred_fake = discriminator(fake_images.detach()).view(-1)\n",
        "            fake_loss = adversarial_loss(discr_pred_fake, fake_labels)\n",
        "            \n",
        "            discr_loss = 0.5 * (real_loss + fake_loss)\n",
        "            discr_loss.backward()\n",
        "            optimizer_D.step()\n",
        "            \n",
        "            # Train Generator\n",
        "            optimizer_G.zero_grad()\n",
        "            discr_pred_fake = discriminator(fake_images).view(-1)\n",
        "            gener_loss = adversarial_loss(discr_pred_fake, real_labels)  # Flip labels\n",
        "            gener_loss.backward()\n",
        "            optimizer_G.step()\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}]')\n",
        "                print(f'D Loss: {discr_loss.item():.4f}, G Loss: {gener_loss.item():.4f}')\n",
        "```\n",
        "\n",
        "### VAE Training Loop\n",
        "```python\n",
        "def train_vae(model, train_loader, epochs, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        recon_loss_total = 0.0\n",
        "        kl_loss_total = 0.0\n",
        "        \n",
        "        for batch_idx, (images, _) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            encoded, z_mean, z_log_var, decoded = model(images)\n",
        "            \n",
        "            # KL Divergence\n",
        "            kl_div = -0.5 * torch.sum(1 + z_log_var - z_mean**2 - torch.exp(z_log_var), axis=1)\n",
        "            kl_div = kl_div.mean()\n",
        "            \n",
        "            # Reconstruction Loss (MSE)\n",
        "            pixelwise = F.mse_loss(decoded, images, reduction='none')\n",
        "            pixelwise = pixelwise.view(images.size(0), -1).sum(axis=1).mean()\n",
        "            \n",
        "            loss = pixelwise + kl_div\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            recon_loss_total += pixelwise.item()\n",
        "            kl_loss_total += kl_div.item()\n",
        "        \n",
        "        print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "        print(f'Total Loss: {total_loss/len(train_loader):.4f}')\n",
        "        print(f'Recon Loss: {recon_loss_total/len(train_loader):.4f}, KL Loss: {kl_loss_total/len(train_loader):.4f}')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
