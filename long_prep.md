Alright, letâ€™s level this up: fewer questions, but each with a very rich, exam-style explanation â€“ the kind your lecturer would be happy to read in a written final.

Iâ€™ll number from Q105 onward.

â¸»

Q105. Explain the full pipeline of Faster R-CNN. Why is it faster and more accurate than earlier R-CNN and Fast R-CNN?

Answer (what you should write in an exam):

Faster R-CNN is a two-stage object detector where both region proposal generation and final detection are done by a single, shared deep network.

1. Backbone feature extraction
	â€¢	Input image is passed through a CNN backbone (e.g., VGG, ResNet) to produce a feature map.
	â€¢	This feature map is shared by both the Region Proposal Network (RPN) and the detection head, which is the key to efficiency.

So instead of repeatedly running convolution on each candidate region (as in R-CNN), we do it just once per image.

â¸»

2. Region Proposal Network (RPN)

The RPN is a small network slid over the backbone feature map.

At each spatial location of the feature map:
	1.	We place multiple anchor boxes with different scales and aspect ratios (e.g., 3 scales Ã— 3 ratios = 9 anchors).
	2.	For each anchor, the RPN predicts:
	â€¢	Objectness score: probability that this anchor corresponds to an object (foreground) vs background.
	â€¢	Bounding box regression offsets: (Î”x, Î”y, Î”w, Î”h) to refine the anchor into a better proposal.

The RPN is typically implemented as:
	â€¢	A 3Ã—3 conv over the feature map to produce an intermediate feature.
	â€¢	Two sibling 1Ã—1 conv layers:
	â€¢	One outputs 2 scores per anchor (object / not object).
	â€¢	One outputs 4 regression values per anchor.

Training the RPN:
	â€¢	Anchors with high IoU with ground-truth boxes (e.g., > 0.7) are labeled positive.
	â€¢	Anchors with very low IoU (e.g., < 0.3) are negative.
	â€¢	Others may be ignored.
	â€¢	RPN is trained with a multi-task loss: classification (objectness) + bounding box regression.

After inference:
	â€¢	The RPN generates many proposals per image (e.g., thousands), then applies NMS to keep the top ~300 proposals.

â¸»

3. ROI pooling / ROI Align and detection head

The selected proposals are treated as Regions of Interest (ROIs):
	1.	Each ROI is projected from image coordinates into the shared feature map coordinates.
	2.	ROI Pooling / ROI Align extracts a fixed-size feature (e.g., 7Ã—7) from the variable-size ROI region.
	3.	This fixed-size feature is flattened and passed through fully-connected layers.

The detection head outputs:
	â€¢	Softmax classification over (C+1) classes (C object classes + background).
	â€¢	Bounding box regression for each class (or class-agnostic in some variants).

At test time, for each class:
	â€¢	Apply the class-specific offsets to each ROI to get refined boxes.
	â€¢	Apply NMS per class to produce the final set of detections.

â¸»

4. Why Faster R-CNN is â€œfasterâ€ than R-CNN and Fast R-CNN
	â€¢	R-CNN:
	â€¢	Uses Selective Search (handcrafted, CPU-based) to get ~2k region proposals.
	â€¢	Runs the CNN separately for each region â†’ Very slow.
	â€¢	Fast R-CNN:
	â€¢	Still uses Selective Search for proposals.
	â€¢	But runs the CNN once per image; proposals are handled via ROI Pooling on the shared feature map.
	â€¢	Faster than R-CNN, but Selective Search is still a bottleneck.
	â€¢	Faster R-CNN:
	â€¢	Replaces Selective Search with RPN, a fully-convolutional network sharing the backbone.
	â€¢	Proposals are generated by the GPU as part of the network.
	â€¢	End-to-end training: detection and proposal generation are trained jointly, improving both speed and quality of proposals.

As a result, Faster R-CNN achieves state-of-the-art accuracy while being significantly faster than R-CNN / Fast R-CNN.

â¸»

5. Why itâ€™s often more accurate
	â€¢	RPN proposals are learned end-to-end specifically for the detection task.
	â€¢	Feature sharing and joint training allows the backbone to learn features that are good for both proposal generation and classification.
	â€¢	This typically gives better localization and fewer bad proposals than hand-crafted methods.

â¸»

Q106. Compare semantic, instance, and panoptic segmentation. Why is panoptic segmentation more challenging?

Answer:

1. Semantic segmentation
	â€¢	Goal: assign a class label to every pixel in the image.
	â€¢	All pixels belonging to the same class share the same label.
	â€¢	Example: all road pixels = â€œroadâ€, all sky pixels = â€œskyâ€.
	â€¢	It does not distinguish between different instances of the same class.
	â€¢	Five people standing together will all be labeled â€œpersonâ€, but the model does not know which pixels belong to person #1 vs person #2.

Typical networks: FCN, U-Net, DeepLab, etc.

â¸»

2. Instance segmentation
	â€¢	Goal: detect each individual object and produce a binary mask for each one.
	â€¢	This combines detection and segmentation:
	â€¢	Assign each pixel to either background or one of the object instances.
	â€¢	The model must:
	1.	Detect objects (like object detection).
	2.	Produce a fine-grained mask for each one.

Example: Mask R-CNN:
	â€¢	Uses Faster R-CNN backbone.
	â€¢	Adds a small mask head per ROI to produce a per-instance mask.

Result: you know that one region is â€œperson #1â€, another is â€œperson #2â€, etc.

â¸»

3. Panoptic segmentation
	â€¢	Panoptic segmentation tries to unify semantic and instance segmentation.
	â€¢	Output:
	â€¢	A single label map where each pixel is assigned:
	â€¢	A semantic class (e.g., road, sky, car, person)
	â€¢	And, if the pixel belongs to a â€œthingâ€ (countable object like person, car), an instance ID.
	â€¢	Classes are typically split into:
	â€¢	â€œStuffâ€ classes: amorphous regions you donâ€™t count (road, sky, grass).
	â€¢	â€œThingâ€ classes: countable objects (car, person, dog).

Panoptic segmentation must:
	1.	Segment all stuff regions (semantic).
	2.	Segment all object instances (instance).
	3.	Merge them into a single consistent labeling with no overlaps or gaps.

â¸»

4. Why panoptic segmentation is more challenging
	â€¢	It requires solving two problems simultaneously:
	â€¢	Semantic segmentation of stuff.
	â€¢	Instance segmentation of things.
	â€¢	Need to handle â€œconflictsâ€:
	â€¢	If an area is labeled as both road and car instance, you must decide which label to keep.
	â€¢	Masks of different instances cannot overlap in the final panoptic map.
	â€¢	Must produce a complete labeling:
	â€¢	Every pixel in the image must belong to exactly one category (stuff or a specific instance).
	â€¢	Evaluation is more complex:
	â€¢	Panoptic Quality (PQ) metric combines segmentation quality (IoU) and detection quality (TP/FP/FN of instances).

Thus, panoptic segmentation demands accurate dense prediction and correct instance separation, plus a consistent merging of outputs â€“ making it conceptually and practically harder than just semantic or instance segmentation alone.

â¸»

Q107. Explain Gaussian Mixture Model (GMM) background subtraction. Why is it better than a single Gaussian per pixel?

Answer:

1. Motivation

In basic background subtraction, we might model each pixelâ€™s intensity as a single Gaussian over time. This assumes:
	â€¢	Background intensity is â€œapproximately constantâ€ with some noise.

But in real scenes, a pixel might switch between several typical intensities:
	â€¢	Tree leaves moving in the wind (light vs dark leaf positions).
	â€¢	Water surfaces with different reflections.
	â€¢	Shadows passing over the ground.
	â€¢	A spot where a car sometimes covers the road.

So the distribution at one pixel can be multi-modal (multiple peaks), not just single-peaked.

â¸»

2. GMM model at each pixel

Instead of one Gaussian, we model the intensity (or color vector) at each pixel as a mixture of K Gaussians:

p(I_t) = \sum_{k=1}^{K} w_k \, \mathcal{N}(I_t; \mu_k, \Sigma_k)

Where at this pixel:
	â€¢	K is small (e.g., 3â€“5).
	â€¢	Each component k has:
	â€¢	Weight w_k (how often it appears).
	â€¢	Mean \mu_k (typical color/intensity).
	â€¢	Covariance \Sigma_k (variance/shape).

Interpretation:
	â€¢	Some Gaussians correspond to background states (frequent, low variance).
	â€¢	Others correspond to foreground events or rare phenomena (low weight).

â¸»

3. Online update per frame

For each new pixel observation I_t:
	1.	Match: Find if it fits an existing Gaussian
	â€¢	Typically if |I_t - \mu_k| < c \cdot \sigma_k for some threshold c.
	2.	If a match is found:
	â€¢	Update that Gaussian using an exponential moving average for mean/variance and weight.
	3.	If no match:
	â€¢	Replace the Gaussian with smallest weight with a new component centered at I_t with high variance, small weight.

Weights w_k are also updated over time, and renormalized so \sum_k w_k = 1.

â¸»

4. Deciding background vs foreground

After fitting or updating all Gaussians at a pixel, we must decide which components represent the background.

Heuristic:
	1.	Sort components by w_k / \sigma_k (high weight, low variance = stable).
	2.	Accumulate weights from the top until the running sum exceeds a threshold T (e.g., 0.7â€“0.9).
	3.	These components are considered background Gaussians.

Now for the current pixel value I_t:
	â€¢	If it matches one of the background Gaussians â†’ background.
	â€¢	Else â†’ foreground.

â¸»

5. Why this is better than a single Gaussian

Single Gaussian per pixel:
	â€¢	Assumes one â€œtypicalâ€ background color â€“ fails if the background alternates between multiple states.
	â€¢	Example: a pixel looking at a waving tree branch; intensity jumps between leafy (dark green) and sky (light blue). One Gaussian would be too broad or always misclassify transitions.

GMM:
	â€¢	Captures multiple typical states at one pixel with distinct \mu_k.
	â€¢	Can model dynamic backgrounds:
	â€¢	Some Gaussians represent different backgrounds.
	â€¢	A car that stays long enough may even become a temporary background component.
	â€¢	More robust to noise, reflections, and natural motion.

Thus, GMM background modeling is much better suited for real-world scenes where background is not truly static.

â¸»

Q108. Describe the Kalman filter and its role in object tracking. What assumptions does it make?

Answer:

The Kalman filter is a recursive estimator used to track the state of a system over time in the presence of noise. In object tracking, itâ€™s used to estimate an objectâ€™s position and velocity across frames.

1. State-space model

We define a state vector x_t at time t, e.g. for 2D tracking:

x_t = [x, y, v_x, v_y]^T

where (x, y) is position, (v_x, v_y) is velocity.

We have two equations:
	1.	State transition (motion model)
x_t = A x_{t-1} + w_t
	â€¢	A: state transition matrix (often constant), e.g. constant velocity model.
	â€¢	w_t: process noise âˆ¼ N(0, Q).
	2.	Measurement model
z_t = H x_t + v_t
	â€¢	z_t: measurement (e.g., observed bounding box center).
	â€¢	H: measurement matrix (e.g., picks out position from state).
	â€¢	v_t: measurement noise âˆ¼ N(0, R).

Assumptions:
	â€¢	Linear dynamics (A and H are linear).
	â€¢	Gaussian noise (w_t, v_t are zero-mean Gaussian).
	â€¢	The goal: estimate x_t given measurements up to time t.

â¸»

2. Prediction and update steps

At each time step, it alternates between:

(a) Prediction
Using previous estimated state \hat{x}_{t-1} and covariance P_{t-1}:

\hat{x}_{t|t-1} = A \hat{x}_{t-1}
P_{t|t-1} = A P_{t-1} A^T + Q

This predicts the prior state and its uncertainty before seeing the new measurement.

(b) Update (correction)
Given new measurement z_t:
	â€¢	Compute Kalman gain:
K_t = P_{t|t-1} H^T (H P_{t|t-1} H^T + R)^{-1}
	â€¢	Update state estimate:
\hat{x}_t = \hat{x}_{t|t-1} + K_t (z_t - H \hat{x}_{t|t-1})
	â€¢	Update covariance:
P_t = (I - K_t H) P_{t|t-1}

Intuition:
	â€¢	If measurement noise R is small (trusted), K is high: we trust measurement more.
	â€¢	If process noise Q is small (we trust motion model), K is low: we trust prediction more.

â¸»

3. Role in object tracking

In tracking-by-detection:
	â€¢	Detector gives us noisy measurements of object positions in each frame.
	â€¢	We use Kalman filter to:
	1.	Predict where each object will be in the next frame â†’ helps association and handle missing detections.
	2.	Smooth noisy observations â†’ stable trajectories instead of jittery bounding boxes.

When a detection is missing in some frame:
	â€¢	We can still propagate the state using the prediction step only, so the tracker â€œcoastsâ€ for a short time, handling short occlusions.

â¸»

4. Key assumptions and limitations

Assumptions:
	â€¢	Linear motion model.
	â€¢	Gaussian noise.
	â€¢	State distribution is fully described by mean and covariance (Gaussian assumption).

Limitations:
	â€¢	Non-linear motion (e.g., sudden turns) may not be captured well.
	â€¢	If noise is non-Gaussian or if you have complex measurement functions, extended/unscented Kalman filters or particle filters are needed.
	â€¢	Kalman filter alone does not solve data association â€“ it predicts states but you still need to match detections to tracks.

In summary, the Kalman filter is a powerful and efficient tool for predicting and smoothing object states in tracking, under linear-Gaussian assumptions.

â¸»

Q109. Explain the architecture and key ideas of PointNet for 3D point cloud processing.

Answer:

PointNet is a neural network designed to work directly on raw point clouds, which are unordered sets of 3D points.

1. Challenge with point clouds
	â€¢	A point cloud is a set \{p_i\}_{i=1}^N where each p_i \in \mathbb{R}^3 (or includes features).
	â€¢	Unordered: there is no fixed order; permuting points shouldnâ€™t change the output.
	â€¢	Traditional CNNs expect data on regular grids (images, voxels).

So we need a network that is permutation invariant and can handle variable-sized sets.

â¸»

2. PointNetâ€™s main idea: symmetric function

To achieve permutation invariance:
	1.	Apply the same function (MLP with shared weights) to each point individually:
h_i = f(p_i; \theta)
This maps each point to a high-dimensional feature vector h_i.
	2.	Aggregate all point features using a symmetric function (order-invariant), typically max pooling:
g = \max_{i} h_i
where max is taken element-wise over the feature dimension.
	3.	The global feature vector g represents the entire point cloud, independent of the order of points.

Because max is symmetric (doesnâ€™t care about order), this ensures permutation invariance.

â¸»

3. Architecture outline

For classification:
	1.	Input: N points (e.g., 1024 points Ã— 3 coordinates).
	2.	Shared MLP per point: e.g., 3 â†’ 64 â†’ 128 â†’ 1024.
	3.	Global max pooling over N points â†’ 1024-D global feature.
	4.	Fully-connected layers: 1024 â†’ 512 â†’ 256 â†’ number of classes.
	5.	Use softmax for class probabilities.

For segmentation:
	â€¢	Combine global feature with per-point features to output per-point labels.
	â€¢	Each point gets concatenated [local features, global feature], then passed through another MLP to produce per-point class scores.

â¸»

4. Spatial transformer networks (T-Nets)

PointNet also includes learned transformations:
	â€¢	Input T-Net: predicts a 3Ã—3 transform matrix to align input points to a canonical pose.
	â€¢	Feature T-Net: predicts a transform in feature space.

These are small networks that output transformation matrices, learned end-to-end, helping the model become invariant to rotations or other transformations.

â¸»

5. Why PointNet is important
	â€¢	First widely adopted method to process point clouds directly, without voxelization or meshing.
	â€¢	Invariant to permutation of points.
	â€¢	Can handle variable-sized point sets.
	â€¢	Simple and efficient: just MLPs and max-pooling.

Limitations:
	â€¢	Ignores local structure: treats points independently, then aggregates.
	â€¢	Later works (PointNet++, DGCNN) explicitly model local neighborhoods and hierarchical features.

â¸»

Q110. Derive and interpret the VAE loss (ELBO). Why do we have a reconstruction term and a KL term?

Answer:

The Variational Autoencoder (VAE) is a generative model that learns to approximate the data distribution p(x) using latent variables z.

1. Generative model setup

We assume:
	â€¢	Prior over latent variables: p(z), typically \mathcal{N}(0, I).
	â€¢	Likelihood model: p_\theta(x|z), parameterized by decoder network \theta.

The true posterior p(z|x) is intractable for complex models.

â¸»

2. Variational approximation

Introduce an approximate posterior q_\phi(z|x) (encoder network), usually Gaussian with mean and variance depending on x.

We want to maximize the data log-likelihood:

\log p_\theta(x)

But this is intractable directly, so we derive a lower bound, the Evidence Lower BOund (ELBO).

â¸»

3. ELBO derivation (sketch)

Start from:

\log p_\theta(x) = \log \int p_\theta(x, z) \, dz

Introduce q_\phi(z|x):

\log p_\theta(x) = \log \int q_\phi(z|x) \frac{p_\theta(x,z)}{q_\phi(z|x)} dz

Apply Jensenâ€™s inequality:

\log p_\theta(x) \ge \int q_\phi(z|x) \log \frac{p_\theta(x,z)}{q_\phi(z|x)} dz

Define the ELBO:

\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x, z) - \log q_\phi(z|x)]

Now expand p_\theta(x, z) = p_\theta(x|z) p(z):

\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}[\log p(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)]

Recognize the KL divergence:

\text{KL}(q_\phi(z|x) \| p(z)) = \mathbb{E}_{q_\phi}[\log q_\phi(z|x) - \log p(z)]

Thus:

\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \text{KL}(q_\phi(z|x) \| p(z))

This is the VAE loss to maximize.

In practice, we minimize the negative ELBO:

\mathcal{L}_{\text{VAE}}(x) = - \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \text{KL}(q_\phi(z|x) \| p(z))

â¸»

4. Interpret the two terms
	1.	Reconstruction term:
- \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]
	â€¢	Encourages the decoder to reconstruct x well from latent z.
	â€¢	If we use Gaussian likelihood, this becomes essentially an L2 reconstruction loss.
	â€¢	If we use Bernoulli likelihood for pixels, this becomes cross-entropy.
	2.	KL regularization term:
\text{KL}(q_\phi(z|x) \| p(z))
	â€¢	Encourages the approximate posterior q_\phi(z|x) to be close to the prior p(z).
	â€¢	Prevents the encoder from mapping each x to an arbitrary, disjoint region of latent space.
	â€¢	Makes the latent space smooth and structured, enabling meaningful sampling by drawing z from p(z).

So the VAE trades off:
	â€¢	Reconstruction quality (data fitting).
	â€¢	Latent space regularity (keeping z near a simple prior).

â¸»

5. Why do we need the KL term?

If we only minimized reconstruction loss:
	â€¢	Encoder could map each input to a separate, distant region of latent space (overfitting).
	â€¢	Sampling z from the prior (e.g., N(0, I)) would not correspond to regions where encoder placed any data â†’ generated samples meaningless.

The KL term couples all q(z|x) distributions to the same prior:
	â€¢	Ensures that the regions of latent space used during training are consistent with the distribution from which we will sample at generation time.
	â€¢	Enables interpolation between points in latent space to give smooth transitions in output space.

â¸»

If youâ€™d like, next I can:
	â€¢	Do the same kind of long, exam-style explanation for GAN pitfalls and WGAN, or
	â€¢	A big conceptual question like â€œCompare FCN, U-Net, and DeepLab for segmentation, in detail.â€

Nice, letâ€™s treat this like two big â€œ10â€“15-pointâ€ exam questions:
	â€¢	Q111 â€“ GAN pitfalls / problems
	â€¢	Q112 â€“ WGAN: what it changes and why it helps

â¸»

Q111. What are the main training pitfalls of standard GANs, and why do they occur?

Answer:

A vanilla GAN is defined by a minâ€“max game between a generator G and a discriminator D:

\min_G \max_D \; V(D,G) = \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]

While this looks simple, training is notoriously unstable. Common pitfalls include:

â¸»

1. Mode collapse

What is it?
	â€¢	The generator produces limited variety of outputs.
	â€¢	Different noise vectors z map to very similar or even identical samples.
	â€¢	E.g., on digits, G generates only â€œ3â€ and â€œ7â€ despite many other digits in the dataset.

Why does it happen?
	1.	The objective is adversarial, not directly enforcing coverage of all data modes.
	2.	Generator updates are driven by gradients from D:
	â€¢	If G finds a few outputs that successfully fool D, gradients may encourage those same outputs over and over.
	3.	The generator can â€œchaseâ€ moving weaknesses of D:
	â€¢	D improves on current fake modes â†’ G shifts to some other narrow region that still fools D â†’ repeat.
	â€¢	This can lead to cycling between few modes.

Intuition:
	â€¢	The GAN loss does not explicitly say â€œcover all modes of the data distributionâ€; it just says â€œproduce something that D thinks is realâ€. Thatâ€™s a much weaker constraint and can be satisfied by focusing on only a subset of modes.

â¸»

2. Training instability and non-convergence

What is it?
	â€¢	Instead of converging to a stable equilibrium (G^*, D^*), training:
	â€¢	Oscillates (loss curves move up and down).
	â€¢	Diverges (loss blows up).
	â€¢	Seemingly â€œnever stabilizesâ€.

Why does it happen?
	1.	Game dynamics, not simple minimization
	â€¢	GAN training is a two-player game, not a simple minimization of one scalar function.
	â€¢	Standard gradient descent on both players can cause cycles (like rockâ€“paperâ€“scissors) rather than convergence.
	2.	Unbalanced training between G and D
	â€¢	If D is too strong:
	â€¢	D(x) \approx 1, D(G(z)) \approx 0 â†’ generatorâ€™s gradients can vanish (see next section).
	â€¢	If D is too weak:
	â€¢	Provides poor feedback; G canâ€™t learn meaningful features.
	3.	Sensitive hyperparameters and architectures
	â€¢	Learning rates, batch size, architecture depth, normalization, etc. heavily affect stability.
	â€¢	Slight mis-tuning often leads to divergence or collapse.

Conclusion:
GAN optimization is not convexâ€“concave; we donâ€™t have strong convergence guarantees, so practical training is fragile.

â¸»

3. Vanishing gradients (especially early in training)

Look at the generatorâ€™s gradient from the original loss:

\mathbb{E}_{z}[\log(1 - D(G(z)))]

If the discriminator becomes very good:
	â€¢	For real data, D(x) \to 1.
	â€¢	For generated data, D(G(z)) \to 0.

Then the term \log(1 - D(G(z))) saturates:
	â€¢	When D(G(z)) is near 0,
\log(1 - D(G(z))) \approx \log(1) = 0
and its derivative w.r.t. Gâ€™s parameters is tiny.

So G receives almost no gradient â†’ training stalls.

A common practical fix is to use the non-saturating generator loss:

\min_G - \mathbb{E}_z[\log D(G(z))]

which gives stronger gradients when D is good, but this is still heuristic and doesnâ€™t fully solve stability issues.

â¸»

4. Discriminator overfitting and poor generalization

What is it?
	â€¢	Discriminator might just memorize the training data, especially with small datasets and large models.
	â€¢	Then D easily separates real vs fake for training samples but provides poor generalization feedback for G.

Why is it bad?
	â€¢	G learns to exploit specific weaknesses of this overfit D rather than approximating the true data distribution.
	â€¢	This can worsen mode collapse and instability.

â¸»

5. Lack of a meaningful, smooth training signal (JS divergence issue)

The original GAN is related to minimizing the Jensenâ€“Shannon (JS) divergence between p_\text{data} and p_G.
	â€¢	Problem: JS divergence is not smooth when the supports of the distributions are disjoint or nearly disjoint (common at the start of training).
	â€¢	When distributions are far apart:
	â€¢	The optimal discriminator saturates (perfectly separates real and fake).
	â€¢	Gradients of JS w.r.t. G vanish â†’ again, poor learning signal.

This â€œall-or-nothingâ€ behavior of JS divergence is a core theoretical reason for unstable GAN training.

â¸»

6. Evaluation is difficult

Not exactly a training pitfall, but important conceptually:
	â€¢	No simple, universally accepted scalar metric like â€œlog-likelihoodâ€ that can be computed easily.
	â€¢	Visual quality can be misleading; overfitted generators may look good yet lack diversity.
	â€¢	Metrics like Inception Score (IS) and FID help but are proxies.

This makes it hard to know:
	â€¢	Whether training is improving.
	â€¢	When to stop training.
	â€¢	Which hyperparameters are best.

â¸»

7. Sensitivity to architecture and hyperparameters

GANs are more sensitive than many other models to:
	â€¢	Learning rate of G vs D.
	â€¢	Update ratio (e.g., 1 step for G, 1 or 5 steps for D).
	â€¢	Choice of normalization (BatchNorm, SpectralNorm).
	â€¢	Choice of activation functions.
	â€¢	Initialization.

Bad choices often lead to divergence or severe mode collapse.

â¸»

Summary for exams:

Standard GANs suffer from mode collapse, vanishing gradients, training instability and non-convergence, sensitivity to hyperparameters, and difficult evaluation. A key theoretical issue is that they effectively minimize JS divergence, which provides poor gradients when the model and data distributions are far apart. This motivates alternative objectives such as the Wasserstein GAN.

â¸»

Q112. What is Wasserstein GAN (WGAN)? How does it address some of the pitfalls of standard GANs?

Answer:

WGAN changes both the objective function and the discriminator to tackle gradient issues and mode collapse, based on the Earth-Mover (Wasserstein-1) distance.

â¸»

1. From JS divergence to Earth-Mover (Wasserstein) distance

Instead of trying to minimize some f-divergence (like JS) between p_\text{data} and p_G, WGAN aims to minimize the Wasserstein-1 distance:

W(p_\text{data}, p_G) = \inf_{\gamma \in \Pi(p_\text{data}, p_G)} \mathbb{E}_{(x,y)\sim \gamma}[\|x - y\|]
	â€¢	Intuitively: the minimum â€œcost of transporting massâ€ to transform one distribution into the other.

Why is this good?
	â€¢	Wasserstein distance is continuous and differentiable almost everywhere with respect to the generator parameters, even when distributions have disjoint supports.
	â€¢	Gives meaningful gradients even when G is very far from the data distribution.

This addresses the â€œzero gradient when distributions donâ€™t overlapâ€ problem of JS.

â¸»

2. Kantorovichâ€“Rubinstein duality and the critic

Computing the infimum over couplings \gamma is intractable. WGAN uses the dual form:

W(p_\text{data}, p_G) = \sup_{\|f\|_L \le 1} \mathbb{E}_{x \sim p_\text{data}}[f(x)] - \mathbb{E}_{x \sim p_G}[f(x)]
	â€¢	The supremum is over all 1-Lipschitz functions f (functions whose gradients are bounded by 1).

In practice:
	â€¢	We approximate such a function f by a neural network D_w, but it is now called a critic, not a classifier:
	â€¢	It no longer outputs â€œprobability real vs fakeâ€.
	â€¢	Instead, it outputs a real score indicating how â€œrealâ€ the sample is.

The WGAN objective becomes:

\max_{w \in \mathcal{W}} \mathbb{E}_{x \sim p_\text{data}}[D_w(x)] - \mathbb{E}_{z \sim p_z}[D_w(G_\theta(z))]

subject to D_w being 1-Lipschitz.

Generator now minimizes this criticâ€™s estimate:

\min_\theta \mathbb{E}_{z \sim p_z}[D_w(G_\theta(z))]

So:
	â€¢	Critic tries to maximize the difference between real and fake scores.
	â€¢	Generator tries to make fake scores as high as real ones, effectively reducing the Wasserstein distance.

â¸»

3. Enforcing the 1-Lipschitz constraint: weight clipping (original WGAN)

To ensure D_w is 1-Lipschitz, original WGAN simply clips the weights of the critic to a small range, e.g., [-c, c] for some c (like 0.01).

Pros:
	â€¢	Very simple to implement.
	â€¢	Works better than vanilla GAN in many cases.

Cons:
	â€¢	Weight clipping is a crude approximation:
	â€¢	Can cause capacity underuse (critic becomes too simple).
	â€¢	Leads to optimization issues (e.g., exploding or vanishing gradients for the critic).

This is why WGAN-GP (with gradient penalty) was later proposed, but more on that below.

â¸»

4. Why WGAN helps with training stability and gradients

Key improvements:
	1.	Non-saturating, meaningful gradient even when far from optimum
	â€¢	Wasserstein distance changes smoothly as generator parameters change, even if p_G and p_\text{data} barely overlap.
	â€¢	The criticâ€™s gradient with respect to Gâ€™s parameters does not vanish so easily as in JS.
	2.	Training curves correlate with sample quality
	â€¢	The estimated Wasserstein distance (critic loss) tends to decrease steadily as training improves.
	â€¢	Unlike the original GAN loss, which can be hard to interpret, WGANâ€™s critic loss often correlates with visual improvement.
	3.	Reduced mode collapse (in practice)
	â€¢	Because the Wasserstein distance is sensitive to how mass is moved, collapsing all mass onto a few modes tends to keep the distance large.
	â€¢	In practice, WGAN often shows much less severe mode collapse than vanilla GAN (though mode collapse can still occur).

â¸»

5. Practical training algorithm (original WGAN)

Typical training loop:
	1.	For each generator update:
	â€¢	Perform n_critic updates (e.g., 5) of the critic:
	â€¢	Sample real data x ~ p_\text{data}.
	â€¢	Sample noise z ~ p_z; generate G(z).
	â€¢	Maximize D(x) - D(G(z)) w.r.t. critic parameters w.
	â€¢	Clip the weights of critic to [-c, c].
	2.	After n_critic steps:
	â€¢	Update generator by minimizing -\mathbb{E}_z[D(G(z))] (or equivalently, minimizing the negative of the critic score on fake data).

This multi-step critic training ensures the critic stays relatively near optimum, approximating the Wasserstein distance better.

â¸»

6. WGAN-GP (gradient penalty) â€“ an important extension

Weight clipping is a rough method for enforcing Lipschitzness. WGAN-GP proposes a better approach:
	â€¢	Add a gradient penalty term to the critic loss:

\lambda \, \mathbb{E}_{\hat{x}} \Big[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2\Big]

where \hat{x} are points sampled along straight lines between real and generated samples.

Interpretation:
	â€¢	Encourages the gradient norm of D w.r.t. its input to be 1 along these lines, which is a stronger and more direct way to approximate 1-Lipschitzness.

Benefits:
	â€¢	More stable training.
	â€¢	Better critic capacity (no need to squash weights by clipping).
	â€¢	Generally better sample quality and robustness to hyperparameters.

In many applications, when people say â€œWGANâ€ they actually mean WGAN-GP because of these benefits.

â¸»

7. Remaining limitations of WGAN

WGAN improves stability but is not a magic bullet:
	â€¢	GAN training remains a game between G and D; oscillations can still occur.
	â€¢	Mode collapse is reduced but not guaranteed away.
	â€¢	Critic still needs careful tuning (architecture, n_critic, learning rates).
	â€¢	Wasserstein distance estimation is only approximate (critic capacity limitations, imperfect Lipschitz enforcement).

However, compared to vanilla GAN, WGAN gives:
	â€¢	More informative training signals.
	â€¢	Better-behaved losses.
	â€¢	Often more robust training.

â¸»

Short exam-style summary:

Standard GANs minimize a JS-divergenceâ€“based objective, leading to vanishing gradients and unstable training when real and generated distributions are far apart. WGAN replaces this with the Earth-Mover (Wasserstein-1) distance, using a critic constrained to be 1-Lipschitz. The critic maximizes the difference in scores between real and fake samples, while the generator minimizes this difference. Because the Wasserstein distance is continuous and provides meaningful gradients even when distributions do not overlap, WGAN training is more stable and better correlated with sample quality. Lipschitz constraints are enforced via weight clipping (original WGAN) or gradient penalty (WGAN-GP).

â¸»

If youâ€™d like, next we can do another long-form question like:
	â€¢	â€œCompare FCN, U-Net, and DeepLab for segmentationâ€, or
	â€¢	â€œExplain different GAN variants: DCGAN, conditional GAN, CycleGAN, etc., and their key ideas.â€

Alright, double long-form exam answers coming up ðŸ‘‡

Iâ€™ll label them:
	â€¢	Q113 â€“ Compare FCN, U-Net, DeepLab for segmentation
	â€¢	Q114 â€“ Key GAN variants: DCGAN, conditional GAN, CycleGAN (plus quick mentions of a few others)

â¸»

Q113. Compare FCN, U-Net, and DeepLab for semantic segmentation. Discuss their architectures, strengths, and weaknesses.

Exam-style answer:

Semantic segmentation = assign a label to every pixel in the image. FCN, U-Net, and DeepLab are three key families of architectures that attack this problem in different ways.

â¸»

1. Fully Convolutional Networks (FCN)

Core idea:
Take a standard classification CNN (e.g., VGG, ResNet), remove the fully-connected layers, and turn them into convolutional layers so the network outputs spatial feature maps instead of a single vector. Then upsample back to input resolution.

Architecture:
	1.	Encoder (backbone):
	â€¢	Start from an image, apply convolution + pooling repeatedly.
	â€¢	Spatial resolution shrink steps: e.g., 224Ã—224 â†’ 112 â†’ 56 â†’ 28 â†’ 14 â†’ 7 (for classification).
	â€¢	Each step reduces size, increases channels.
	2.	Replace FC with 1Ã—1 conv:
	â€¢	The classification layers are replaced by 1Ã—1 convolutions that map from C channels to num_classes channels over the 7Ã—7 (or similar) feature map.
	â€¢	So you get a low-resolution score map for each class.
	3.	Upsampling / deconvolution:
	â€¢	Use transposed convolution (a learned upsampling) to bring 7Ã—7 â†’ 224Ã—224.
	â€¢	FCN-32s: one big upsampling step.
	â€¢	FCN-16s, FCN-8s: introduce skip connections from earlier, higher-resolution layers to refine details (e.g., combine 7Ã—7 with 14Ã—14 and 28Ã—28).

Strengths:
	â€¢	Simple, end-to-end, fully convolutional.
	â€¢	Reuses standard classification backbones.
	â€¢	First big step in making segmentation efficient compared to sliding windows.

Weaknesses:
	â€¢	Early versions (FCN-32s) were blurry because upsampling from very low resolution.
	â€¢	Even with skip connections, boundaries may be coarse.
	â€¢	No explicit mechanism for multi-scale context beyond what backbone naturally captures.

â¸»

2. U-Net

Originally designed for biomedical image segmentation but widely used in many domains.

Core idea:
Explicit encoderâ€“decoder structure with symmetric skip connections between encoder and decoder at multiple resolutions. The â€œUâ€ shape.

Architecture:
	1.	Encoder (contracting path):
	â€¢	Repeated blocks: 2 Ã— (conv + ReLU), then max-pooling to downsample.
	â€¢	Each downsampling step halves spatial size, doubles channels.
	â€¢	Similar to the encoder part of a typical CNN.
	2.	Decoder (expanding path):
	â€¢	For each step:
	â€¢	Upsampling (transposed conv or bilinear upsample + conv).
	â€¢	Concatenate the corresponding feature map from the encoder (skip connection).
	â€¢	Then apply conv layers to merge.
	3.	Skip connections:
	â€¢	Very important: they pass high-resolution spatial information (edges, fine structures) from encoder to decoder.
	â€¢	Decoder receives both:
	â€¢	High-level semantics (from deeper layers).
	â€¢	High-resolution detail (from shallower layers).
	4.	Final layer:
	â€¢	1Ã—1 conv maps the final decoder output to per-pixel class scores.

Strengths:
	â€¢	Excellent at precise localization (sharp boundaries, small structures).
	â€¢	Very effective for medical images where fine contours matter (e.g., tumors).
	â€¢	Trains well even on relatively small datasets (original paper used heavy data augmentation and still worked).

Weaknesses:
	â€¢	If you use a large backbone and full U-structure, memory usage can be high (lots of feature maps at multiple resolutions).
	â€¢	Basic U-Net (without extra tricks) may not exploit very large receptive fields or complex context as much as more advanced models like DeepLab with atrous convolutions.
	â€¢	The basic architecture doesnâ€™t explicitly model multi-scale context beyond encoderâ€“decoder.

â¸»

3. DeepLab (v1â€“v3+) â€“ Atrous convolution and multi-scale context

DeepLab is a family of models from Google that address two core challenges:
	1.	Need large receptive field (global context) to correctly classify ambiguous pixels.
	2.	Need high resolution to get accurate boundaries (donâ€™t just downsample to 1/32 and then crudely upsample).

Core ingredients:
	â€¢	Atrous (dilated) convolutions
	â€¢	ASPP (Atrous Spatial Pyramid Pooling)
	â€¢	Optional CRF post-processing (in early versions)
	â€¢	In v3+ a lightweight decoder.

3.1 Atrous (dilated) convolution
	â€¢	Normal conv with dilation rate r=1 uses a contiguous kernel.
	â€¢	Atrous conv inserts râˆ’1 zeros between kernel elements:
	â€¢	Increases receptive field without increasing number of parameters or changing feature map resolution.
	â€¢	Example: instead of downsampling by stride=2, you can keep the feature map larger and use atrous conv to capture broad context.

Effect:
	â€¢	You can keep features at 1/8 or 1/16 resolution instead of 1/32, while still having a large receptive field.
	â€¢	Better localization because spatial resolution is higher.

3.2 ASPP â€“ Atrous Spatial Pyramid Pooling
	â€¢	Apply multiple parallel atrous conv layers with different dilation rates (e.g., 1, 6, 12, 18) plus maybe global pooling.
	â€¢	Each branch captures context at a different scale.
	â€¢	Outputs from branches are concatenated and fused.

This creates a multi-scale context representation:
â€“ small dilation â†’ local details
â€“ large dilation â†’ global context.

3.3 DeepLab versions:
	â€¢	DeepLab v1/v2:
	â€¢	Use atrous conv and sometimes DenseCRF as post-processing to refine boundaries.
	â€¢	DeepLab v3:
	â€¢	Emphasizes ASPP (no CRF needed), stronger multi-scale context.
	â€¢	DeepLab v3+:
	â€¢	Adds a simple decoder on top of v3:
	â€¢	Takes ASPP output (low-res but semantically rich) and combines with features from an earlier high-resolution layer.
	â€¢	Upsamples progressively to final resolution.

So DeepLab v3+ is conceptually close to U-Net: an encoderâ€“decoder, but with atrous conv + ASPP in the encoder.

Strengths:
	â€¢	Very strong performance on benchmarks (e.g., PASCAL VOC, COCO).
	â€¢	Atrous conv + ASPP â†’ powerful multi-scale context capture.
	â€¢	v3+ gives good boundary accuracy and large receptive field at the same time.
	â€¢	Scales well with strong backbones (ResNet, Xception).

Weaknesses:
	â€¢	Architecturally more complex than vanilla FCN or U-Net.
	â€¢	Atrous convolutions + ASPP can be computationally heavier.
	â€¢	Implementation details (dilation rates, output stride) are more involved.

â¸»

4. Summary comparison (how to answer in exam form)
	â€¢	FCN:
	â€¢	Key idea: Convert classification network to fully convolutional; upsample coarse feature maps to get segmentation.
	â€¢	Pros: Simple, reusable backbone, end-to-end training.
	â€¢	Cons: Coarse outputs, boundaries often blurry, limited multi-scale context.
	â€¢	U-Net:
	â€¢	Key idea: Symmetric encoderâ€“decoder with skip connections at each resolution level.
	â€¢	Pros: Excellent localization, sharp boundaries, good with small datasets; widely used in medical imaging.
	â€¢	Cons: Memory-heavy; basic version less focused on large receptive field / global context.
	â€¢	DeepLab (v3+):
	â€¢	Key idea: Atrous convolutions to enlarge receptive field without losing resolution; ASPP for multi-scale context; decoder for refinement.
	â€¢	Pros: State-of-the-art performance; strong at both context and boundary accuracy.
	â€¢	Cons: More complex; heavier computationally; more hyperparameters (dilation, output stride).

A strong exam answer clearly describes these architectures, highlights how they handle resolution vs context, and compares their application strengths.

â¸»

Q114. Describe the main ideas behind DCGAN, Conditional GAN, and CycleGAN. How do they extend the basic GAN framework?

Answer:

All of these are variants of the basic GAN that adapt the architecture or the objective to solve specific problems.

Letâ€™s start from the vanilla GAN:
	â€¢	Generator G(z) maps random noise z â†’ fake samples (e.g., images).
	â€¢	Discriminator D(x) outputs probability that x is real.
	â€¢	Objective:
\min_G \max_D \; \mathbb{E}_{x \sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))].

Now weâ€™ll modify this baseline in different ways.

â¸»

1. DCGAN â€“ Deep Convolutional GAN

Problem being addressed:
Early GANs often used fully-connected layers and simple architectures â†’ unstable training and low-quality images.

DCGAN (Deep Convolutional GAN) proposes a set of architectural guidelines for stable GAN training on images using conv nets.

Key architectural ideas:
	1.	Fully convolutional generator:
	â€¢	Input: random noise vector z (e.g., 100-dim).
	â€¢	First layer: fully connected to a small low-res feature map (e.g., 4Ã—4Ã—512).
	â€¢	Then a series of fractionally-strided convolutions (transposed conv) to upsample:
	â€¢	4Ã—4 â†’ 8Ã—8 â†’ 16Ã—16 â†’ 32Ã—32 â†’ 64Ã—64, etc.
	â€¢	Each layer uses conv + batch norm + ReLU (except last layer uses Tanh to output image in [-1, 1]).
	2.	Convolutional discriminator:
	â€¢	Input: image (e.g., 64Ã—64Ã—3).
	â€¢	A stack of strided convolutions (no pooling) to downsample; channels increase.
	â€¢	Conv + batch norm + LeakyReLU.
	â€¢	Final output: single logit â†’ sigmoid â†’ probability real/fake.
	3.	Design â€œrules of thumbâ€ from DCGAN:
	â€¢	Use strided conv instead of pooling (network learns its own down/upsampling).
	â€¢	Use BatchNorm in both G and D (but not on Dâ€™s input and Gâ€™s output).
	â€¢	Use ReLU in G (except Tanh output), LeakyReLU in D.
	â€¢	Avoid fully-connected layers in deeper parts â€“ keep it convolutional.

Impact:
	â€¢	Much more stable training compared to earlier GANs.
	â€¢	Generates decent, recognizable images for datasets like LSUN, CIFAR, faces.
	â€¢	Became a standard template for many later GAN architectures.

â¸»

2. Conditional GAN (cGAN)

Problem being addressed:
Vanilla GAN generates samples unconditionally â€“ no control over which class or style is produced.

Conditional GAN introduces a conditioning variable c (could be class label, text, another image, etc.).

Main idea:
	â€¢	Both G and D receive c as additional input:
	â€¢	Generator: G(z, c) â†’ generates samples consistent with condition c.
	â€¢	Discriminator: D(x, c) â†’ must decide whether x is a real sample with condition c or fake G(z, c).

The objective becomes:

\min_G \max_D \; \mathbb{E}_{x,c \sim p_\text{data}}[\log D(x, c)] + \mathbb{E}_{z \sim p_z, c \sim p(c)}[\log(1 - D(G(z, c), c))]

Implementation details:
	â€¢	For class labels, c is often a one-hot vector:
	â€¢	Concatenate with z and feed into G (e.g., [z; c]).
	â€¢	For D, concatenate c with image at some layer (e.g., replicate c spatially and concatenate along channels).
	â€¢	For image-to-image translation:
	â€¢	c can be an input image (e.g., edge map, segmentation map).
	â€¢	G learns mapping from c â†’ target domain (e.g., pix2pix: edges â†’ photo).
	â€¢	D takes pairs (c, x) and must judge if x is a real output for input c.

Benefits:
	â€¢	Control over outputs: e.g., generate â€œdigit 7â€ or â€œclass: catâ€.
	â€¢	Enables many tasks:
	â€¢	Class-conditional generation.
	â€¢	Image-to-image translation (facades, maps, sketches).
	â€¢	Text-to-image (c = text embeddings).

Drawbacks / challenges:
	â€¢	Must have suitable conditioning information and paired data for some tasks (e.g., pix2pix needs paired images).
	â€¢	Mode collapse and instability are still possible (itâ€™s still a GAN).

â¸»

3. CycleGAN â€“ Unpaired image-to-image translation

Problem being addressed:
Conditional GANs like pix2pix usually assume paired training data: (input, target) image pairs.
In many real situations, paired data is unavailable: e.g., a set of horse photos and a separate set of zebra photos, but no precise horseâ†”zebra pairs.

CycleGAN enables unpaired image-to-image translation between two domains X and Y:
	â€¢	Example: horses â†” zebras, summer â†” winter, Monet paintings â†” photos.

Main components:
Two generators:
	â€¢	G: X \to Y (e.g., horse â†’ zebra)
	â€¢	F: Y \to X (e.g., zebra â†’ horse)

Two discriminators:
	â€¢	D_Y: distinguishes real Y images vs fake G(X).
	â€¢	D_X: distinguishes real X images vs fake F(Y).

Loss functions:
	1.	Adversarial loss for each direction:

	â€¢	For domain Y:

\mathcal{L}_{GAN}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_Y}[\log D_Y(y)] + \mathbb{E}_{x \sim p_X}[\log(1 - D_Y(G(x)))]
	â€¢	For domain X (with F and D_X) similarly.

This encourages G(X) to look like real Y, and F(Y) to look like real X.
	2.	Cycle consistency loss:

	â€¢	Enforces that translate-there-and-back should reconstruct the original:

\mathcal{L}_{cyc}(G, F) = \mathbb{E}_{x \sim X}[\|F(G(x)) - x\|_1] + \mathbb{E}_{y \sim Y}[\|G(F(y)) - y\|_1]

This is the cycle consistency constraint.

Intuition:
	â€¢	If you take a horse image x, apply G to get zebra G(x), then apply F to get back F(G(x)), it should be close to the original horse x.
	â€¢	This discourages G from ignoring the input and mapping all X images to the same Y image (which adversarial loss alone might allow).
	â€¢	It aligns structure between domains (e.g., position of legs, head) even without pairs.

	3.	Full objective:

\mathcal{L}(G,F,D_X,D_Y) = \mathcal{L}_{GAN}(G,D_Y,X,Y) + \mathcal{L}_{GAN}(F,D_X,Y,X) + \lambda \mathcal{L}_{cyc}(G,F)

where Î» controls weight of cycle loss.

Strengths:
	â€¢	Works with unpaired datasets â€“ very attractive in practice.
	â€¢	Preserves image structure (due to cycle loss) while changing â€œstyleâ€ from one domain to another.
	â€¢	Has produced impressive results for style transfer, season change, etc.

Weaknesses:
	â€¢	Cycle consistency is a heuristic: not guaranteed to enforce correct semantics.
	â€¢	Can fail when mapping is inherently ambiguous or one-to-many.
	â€¢	Still inherits GAN issues: mode collapse, sensitivity, etc.

â¸»

4. Quick mentions of other GAN variants (you can drop short bullets in an exam)
	â€¢	WGAN / WGAN-GP:
	â€¢	Replace JS divergence with Wasserstein distance; use critic instead of classifier; gradient penalty.
	â€¢	More stable training, better gradient behavior, less mode collapse.
	â€¢	StyleGAN:
	â€¢	Popular for high-resolution, high-quality face generation.
	â€¢	Style-based generator: latent code injected at multiple layers via adaptive instance normalization (AdaIN).
	â€¢	Controls â€œstyleâ€ at different scales (coarse â†’ pose, fine â†’ texture).
	â€¢	Pix2Pix:
	â€¢	Conditional GAN for paired image-to-image translation.
	â€¢	Uses L1 reconstruction loss + GAN loss to ensure realism and faithfulness to input.
	â€¢	BigGAN:
	â€¢	Large-scale class-conditional GAN for high-resolution ImageNet images.
	â€¢	Uses many advanced tricks (orthogonal regularization, spectral norm, large batch training).

You donâ€™t need full details on all of these, but mentioning them shows you know the landscape.

â¸»

Summary for Q114 (how you might end a written answer):

DCGAN shows how to build stable convolutional GANs for image generation by using strided convs, batch normalization, and specific activation functions. Conditional GANs extend the framework by conditioning both generator and discriminator on extra information (labels, images, text), enabling controlled generation and tasks like image-to-image translation. CycleGAN further extends this to unpaired domains using two generators and two discriminators along with a cycle-consistency loss, allowing mapping between domains without paired training data. Together, these variants illustrate how changing the architecture and objective of GANs can adapt them to different tasks and alleviate some traditional GAN problems.

â¸»

If youâ€™d like next, I can:
	â€¢	Turn all these long answers into short bullet â€œcheat sheetsâ€ you can skim right before the exam, or
	â€¢	Give you a mock written exam mixing CNN, detection, tracking, segmentation, 3D, and generative models (no solutions first, then a second message with full solutions).

